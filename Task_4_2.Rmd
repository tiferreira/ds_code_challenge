---
title: "Task 4.2 - Introspection Challenge"
author: "Thomas Ferreira"
date: "16/05/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Required Packages

```{r}
library(data.table)
```

## Get the data
I download the files to the project folder, uncompress them and remove the compressed versions. If the uncompressed files already exist, there is no need to download them. 

```{r}
if (file.exists("sr_hex.csv")==FALSE) {
  download.file(url="https://cct-ds-code-challenge-input-data.s3.af-south-1.amazonaws.com/sr_hex.csv.gz", destfile ="sr_hex.csv.gz", method='curl')
  untar("sr_hex.csv.gz")
  file.remove("sr_hex.csv.gz")
  }
sr <- fread("sr_hex.csv")
```

##Reshaping
Limit data to the Water and Sanitation department and drop data with no Hex index.
Remove initial sr_hex.csv file to free up memory. Calculate the counts by Hex and Request Type. 

I merge in median property values by suburb from the City's opendata portal as a measure of socio-economic status. I take the mean of proprtyvalues by hex as hex's might overlap with suburb borders. 

I also take the number of requests per hex for the same request and all requests for 2017 and 2018 as explanatory variables. 

```{r}
sr[,month:=month(CreationDate)]
sr[,year:=year(CreationDate)]
water <- sr[department=="Water and Sanitation" & h3_level8_index!=0]
rm(sr)

# Merge in suburb property values
sub_val <- fread("Valuation suburbs 2012 and 2015.csv")
setnames(sub_val, "OFFICIAL_SUBURB", "OfficialSuburbs")
data_sub <- merge(water, sub_val, by="OfficialSuburbs", all.x=TRUE)
hex_val <- data_sub[,mean(GV2015_VAL), by="h3_level8_index"]
setnames(hex_val, "V1", "Propval")

# Get nr of requests for previous years for each hex. 
N_by_year <- dcast(
  data = water[, .N, by=.(h3_level8_index, Code, year, month)],
  formula = h3_level8_index + Code ~ year,
  value.var = "N"
)
setnames(N_by_year, c("2017", "2018", "2019"), c("N2017", "N2018", "N"))

#merge all the data together
data <- merge(N_by_year, hex_val, by="h3_level8_index", all.x=TRUE)
```
  
##  Selecting request types             

Request types that have large variation and a large number of observations are more effectively explained. Thus I want to shy away for request type with low variation. Next I calculate the standard deviation of the counts by request type and identify types with sufficient variation and tabulate request types that have standard deviations in the 25%. Tabulating the data also shows the number of observations by code. I select Customer Reconnection Requests to work with. 

```{r}
data[,N_sd:= sd(N), by="Code"]
summary(data[,N_sd])
table(data[N_sd>=2.536,Code])  
```  
## Count Data Models
Count data should not be modelled with OLS. As part of the GLM family in R, poisson regressions can be used which are more suitable to count data. It is just also important to note if the data is zero-inflated as these require models such as "zero-inflated poisson regression". I check this with a histogram. While the data does have a large number of zeroes, it does not seem to be inflated extremely. It is also inflated at the maximum.  

```{r}
modeldata <- data[Code=="Customer : Reconnection Request",]
hist(modeldata$N)
```
## Poisson Regression Results

The results suggest that all variables included are statistically significant (even though the assumptions underlying the standard errors are mildy violated (see https://stats.idre.ucla.edu/r/dae/poisson-regression/#:~:text=Poisson%20regression%20%E2%80%93%20Poisson%20regression%20is,variance%20exceeds%20the%20conditional%20mean.).  

There seems to be fairly strong correlations between lagged number of reconnection requests per year and reconnecting requests in 2019. Two explanations for this are either that there are a number of repeating customers who fail to pay their accounts and then later request for them to be reconnected. The other is that the general population in hex's with high requests, have fluctuating fortunes in income earned. One would expect some of the variation to be accounted for by the inclusion of average property values but it does not capture all the variation. 

Logged Average property values also have a significant effect on the number of reconnection requests. It is suprising that it is positively correlated with reconnection requests. One would expect areas with higher socio-economic status to have more stable incomes and pay their bills. It is possible that property values have a non-linear relationship wih the number of service requests. Further analysis is needed to expand the model. 

The overall fit of the model still needs to be evaluated.
                 
```{r}
summary(glm(N ~ N2017 + N2018 + log(Propval), family="poisson", data=modeldata))
```
                 
